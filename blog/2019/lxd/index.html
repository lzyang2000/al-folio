<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="TAAflQRckL_Zo0wwDnbOPjlfofaCLr_wtl7x0WO96ok"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Creating Shared GPU Server Based On LXD Containers | Lizhi Yang</title> <meta name="author" content="Lizhi Yang"> <meta name="description" content="a quick run down on shared GPU server creation."> <meta name="keywords" content="lizhi, yang, robotics, control, research, machine learning, tech"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://lzyang2000.github.io/blog/2019/lxd/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Creating Shared GPU Server Based On LXD Containers",
      "description": "a quick run down on shared GPU server creation.",
      "published": "September 8, 2019",
      "authors": [
        {
          "author": "Lizhi Yang",
          "authorURL": "https://lzyang2000.github.io",
          "affiliations": [
            {
              "name": "MCE, Caltech",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Lizhi </span>Yang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Creating Shared GPU Server Based On LXD Containers</h1> <p>a quick run down on shared GPU server creation.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#preface">Preface</a></div> <div><a href="#system-and-driver-installation">System and Driver Installation</a></div> <div><a href="#lxd-installation-and-configuration">LXD Installation and Configuration</a></div> <div><a href="#creating-the-template">Creating the Template</a></div> <div><a href="#container-ssh-access">Container SSH Access</a></div> <div><a href="#container-remote-access-through-frp">Container Remote Access Through frp</a></div> <div><a href="#creating-deploying-new-containers">Creating &amp; Deploying New Containers</a></div> <div><a href="#sources">Sources</a></div> </nav> </d-contents> <h2 id="preface">Preface</h2> <p>Our lab recently purchased a few GPUs and I am tasked with installing them on a workstation. After some research on the web, I decided to go with the LXD container solution, offering nice features such as container isolation, access to GPUs, instant deployment and control over resources. </p> <p>Note - all &lt;…&gt; in this article are to be replaced with actual content in installation and configuration unless stated otherwise. For example, &lt;your ip&gt; would be replaced as 198.12.2.1, that is, your ip.</p> <h2 id="system-and-driver-installation">System and Driver Installation</h2> <h3 id="linux-installation">Linux Installation</h3> <p>I chose to use Ubuntu 18.04 (Bionic Beaver) as the host system as it defaults to installing LXD 3.0, which supports GPUs in containers. The installation itself is trivial, one thing to note however is that during or after installation, we need to create a ext4 partition that is not mounted to anything, and note down its block name (normally /dev/sdax,/dev/sdbx, etc) as we will need this space to store the containers later. The screenshot below is a 5TB HDD in one partition (this is where containers are stored).</p> <h3 id="gpu-driver-installation">GPU Driver Installation</h3> <p>Next comes the installation of the GPU driver - and it seems that the easiest way is to install through the <code class="language-plaintext highlighter-rouge">Software &amp; Updates</code> application in Ubuntu. Reboot after finishing installation.</p> <p>After reboot, type <code class="language-plaintext highlighter-rouge">nvidia-smi</code> to check the version of the installed driver. Note it down for future use in the containers.</p> <p> </p> <h2 id="lxd-installation-and-configuration">LXD Installation and Configuration</h2> <h3 id="lxd-installation">LXD Installation</h3> <p>In terminal, install the required packages,</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">sudo apt-get install lxd zfsutils-linux bridge-utils
</span></code></pre></div></div> <ul> <li>lxd : the container software</li> <li>zfsutils-linux : disk management</li> <li>bridge-utils : bridge the internet to containers <h3 id="lxd-configuration">LXD Configuration</h3> <p>In terminal, initilize LXD,</p> <div class="language-terminal highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="go">sudo lxd init
</span></code></pre></div> </div> <p>The two things that are not default are :</p> </li> <li>Would you like to use an existing block device? (yes/no) [default=no]: yes</li> <li>Path to the existing block device: /your/partition</li> </ul> <p>One thing to note - If your disk/partition is currently formatted and mounted on the system, it will need to be unmounted with <code class="language-plaintext highlighter-rouge">sudo umount /path/to/mountpoint</code> before continuing, or LXD will error during configuration. Additionally if there’s an <code class="language-plaintext highlighter-rouge">fstab</code> entry this will need to be removed before continuing, otherwise you’ll see mount errors when you next reboot.<sup id="a1"><a href="#beaware">1</a></sup></p> <p> </p> <h2 id="creating-the-template">Creating the Template</h2> <h3 id="downloading-the-image-and-creating-a-container">Downloading the Image and Creating a Container</h3> <p>In terminal, list the available images,</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">sudo lxc image list images:
</span></code></pre></div></div> <p>Find the <code class="language-plaintext highlighter-rouge">ubuntu/18.04 (7 more) | 1a6a4b9d3f60 | yes | Ubuntu bionic amd64 (20190908_07:42) | x86_64 | 93.75MB |</code> entry, the second column <code class="language-plaintext highlighter-rouge">1a6a4b9d3f60</code> represents the fingerprint of the image.<br> To download the image and launch the container, in terminal run</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">sudo lxc launch images:&lt;FINGERPRINT&gt;</span><span class="w"> </span>&lt;ContainerTemplateName&gt;
</code></pre></div></div> <p>&lt;FINGERPRINT&gt; is the second column mentioned above and &lt;ContainerTemplateName&gt; is what you want to call the container. In this instance I chose bionicTemplate.</p> <ul> <li>Use <code class="language-plaintext highlighter-rouge">sudo lxc list</code> to see the images</li> <li>Use <code class="language-plaintext highlighter-rouge">sudo lxc exec &lt;ContainerTemplateName&gt; bash</code> to access the root bash of the container. Switch to standard user <code class="language-plaintext highlighter-rouge">ubuntu</code> using <code class="language-plaintext highlighter-rouge">su ubuntu</code> </li> <li>Note - if the container throws the error <code class="language-plaintext highlighter-rouge">sudo: no tty present and no askpass program specified</code>, then edit the file <code class="language-plaintext highlighter-rouge">/etc/sudoers</code>, and add <code class="language-plaintext highlighter-rouge">ubuntu ALL=(ALL) NOPASSWD:ALL</code> to the end of the file<sup id="a2"><a href="#notty">2</a></sup>.</li> </ul> <h3 id="creating-shared-directory-and-sharing-the-gpu">Creating Shared Directory and Sharing the GPU</h3> <h4 id="shared-directory-configuration">Shared Directory Configuration</h4> <p>This is for R/W within the container. If only read permission is suffientm just run</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>Set shared directory，&lt;shareName&gt; is the virtual device name of your wish, &lt;path1&gt; is the shared directory under the host，&lt;path2&gt; is the shared directory under the container that already exsists
<span class="go">
</span><span class="gp">sudo lxc config device add &lt;ContainerTemplateName&gt;</span><span class="w"> </span>&lt;shareName&gt; disk <span class="nb">source</span><span class="o">=</span>&lt;path1&gt; <span class="nv">path</span><span class="o">=</span>&lt;path2&gt;
</code></pre></div></div> <p>In terminal, run</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>Allow the LXD demon <span class="o">(</span>running as root<span class="o">)</span> to remap our host<span class="s1">'s user ID inside a container - one time step
</span><span class="go">
</span><span class="gp">echo "root:$</span>UID:1<span class="s2">" | sudo tee -a /etc/subuid /etc/subgid
</span><span class="go">
</span><span class="gp">#</span><span class="w"> </span>Stop the container 
<span class="go">
</span><span class="gp">sudo lxc stop &lt;ContainerTemplateName&gt;</span><span class="w">
</span><span class="go">
</span><span class="gp">#</span><span class="w"> </span>Remap the UID inside the container - need to <span class="k">do for </span>every container created
<span class="go">
</span><span class="gp">sudo lxc config set &lt;ContainerTemplateName&gt;</span><span class="w">  </span>raw.idmap <span class="s2">"both </span><span class="nv">$UID</span><span class="s2"> 1000"</span>
<span class="go">
</span><span class="gp">#</span><span class="w"> </span>Give the container elevated privileges
<span class="go">
</span><span class="gp">sudo lxc config set &lt;ContainerTemplateName&gt;</span><span class="w"> </span>security.privileged <span class="nb">true</span>
<span class="go">
</span><span class="gp">#</span><span class="w"> </span>Set shared directory，&lt;shareName&gt; is the virtual device name of your wish, &lt;path1&gt; is the shared directory under the host，&lt;path2&gt; is the shared directory under the container that already exsists
<span class="go">
</span><span class="gp">sudo lxc config device add &lt;ContainerTemplateName&gt;</span><span class="w"> </span>&lt;shareName&gt; disk <span class="nb">source</span><span class="o">=</span>&lt;path1&gt; <span class="nv">path</span><span class="o">=</span>&lt;path2&gt;
<span class="go">
</span><span class="gp">#</span><span class="w"> </span>Start the container 
<span class="go">
</span><span class="gp">sudo lxc start &lt;ContainerTemplateName&gt;</span><span class="w">
</span><span class="go">
</span><span class="gp">#</span><span class="w"> </span><span class="o">(</span>Optional<span class="o">)</span>  Access the standard user of the container
<span class="gp">sudo lxc exec &lt;ContainerTemplateName&gt;</span><span class="w"> </span>bash
<span class="go">
su ubuntu
</span></code></pre></div></div> <h4 id="shared-gpu-configuration">Shared GPU Configuration</h4> <p>In terminal, run one of the two commands below,</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>Give container access to all GPUs
<span class="go">
</span><span class="gp">sudo lxc config device add &lt;ContainerTemplateName&gt;</span><span class="w"> </span>gpu gpu
<span class="go">
</span><span class="gp">#</span><span class="w"> </span>Give container access to selected GPU 
<span class="go">
</span><span class="gp">sudo lxc config device add &lt;ContainerTemplateName&gt;</span><span class="w"> </span>gpu0 gpu <span class="nb">id</span><span class="o">=</span>0
</code></pre></div></div> <p>Now we look back on the host’s GPU driver version that we noted down before, download it from the website or wget it from the terminal, version 430.26 is linked <a href="http://us.download.nvidia.com/XFree86/Linux-x86_64/430.26/NVIDIA-Linux-x86_64-430.26.run" rel="external nofollow noopener" target="_blank">here</a> for convenience, you can also run</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">wget http://us.download.nvidia.com/XFree86/Linux-x86_64/430.26/NVIDIA-Linux-x86_64-430.26.run
</span></code></pre></div></div> <p>but please adapt your own version depending on your drivers.</p> <p>Installing the driver inside the container is rather simple, just run</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>Access the bash
<span class="go">
</span><span class="gp">sudo lxc exec &lt;ContainerTemplateName&gt;</span><span class="w"> </span>bash
<span class="go">
</span><span class="gp">#</span><span class="w"> </span>Go to the directory where the driver installation file is kept
<span class="go">
cd /driver/installation/file/directory

</span><span class="gp">#</span><span class="w"> </span>Install the driver without kernel
<span class="go">
sudo sh NVIDIA-Linux-x86_64-xxx.xx.run --no-kernel-module
</span></code></pre></div></div> <p>After installation, use <code class="language-plaintext highlighter-rouge">nvidia-smi</code> to check that it is successful. Notice here that the account is ubuntu and the hostname is bionicTemplate.</p> <p> </p> <h2 id="container-ssh-access">Container SSH Access</h2> <h3 id="configuring-ssh-login">Configuring SSH login</h3> <p>In terminal, install <code class="language-plaintext highlighter-rouge">openssh-server</code></p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span><span class="nb">install</span>
<span class="go">
sudo apt install openssh-server

</span><span class="gp">#</span><span class="w"> </span>check service
<span class="go">
sudo systemctl status ssh

</span><span class="gp">#</span><span class="w"> </span><span class="k">if </span>it is not Active: active <span class="o">(</span>running<span class="o">)</span>, run
<span class="go">
sudo systemctl enable ssh
sudo systemctl start ssh
</span></code></pre></div></div> <p>As our server in in the university network, I did not go over the hassle of configuring firewalls. If you want to, here is an excellent <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-a-firewall-with-ufw-on-ubuntu-18-04" rel="external nofollow noopener" target="_blank">tutorial</a> by Digital Ocean.</p> <p>Next we configure the ssh keys. In the terminal of the container, run</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">cd ~/.ssh

</span><span class="gp">#</span><span class="w"> </span>Set the password to the key at your will
<span class="go">
ssh-keygen -t rsa
</span><span class="gp">sudo cat id_rsa.pub &gt;</span><span class="o">&gt;</span> authorized_keys
<span class="go">sudo service ssh restart
</span></code></pre></div></div> <h3 id="testing-ssh">Testing SSH</h3> <p>In the container terminal, run <code class="language-plaintext highlighter-rouge">ifconfig</code> (if it errors run <code class="language-plaintext highlighter-rouge">sudo apt install net-tools</code> first), note down the ip address. Do the same thing for the host, but this time note down the ip adress for lxdbr0.</p> <p>Next, copy the SSH private key id_rsa to the host through the shared directory that we set up earlier. After copied, put it in a safe place, then in the host terminal, run</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span><span class="nb">set </span>permissions to read-only to prevent accidental overwrites, you can also rename it to anything you want
<span class="go">
sudo chmod 400 id_rsa

</span><span class="gp">#</span><span class="w"> </span>SSH login
<span class="go">
</span><span class="gp">ssh -i id_rsa ubuntu@&lt;the ip of container&gt;</span><span class="w">
</span></code></pre></div></div> <p>Check that we can log in successfully.</p> <p> </p> <h2 id="container-remote-access-through-frp">Container Remote Access Through frp</h2> <h3 id="why-this-part">Why this part?</h3> <p>The reason is though we have successfully configured SSH login, it could only be done on the host machine - which defeats the purpose of this article. Thus we need to configure it such that the connection can “pass through” the host and remote machines can access it. frp is an excellent tool for this purpose. Below is a graph to illustrate the process from their GitHub<sup id="a3"><a href="#frp">3</a></sup>.</p> <h3 id="installing-frp">Installing frp</h3> <p>Download the latest release from <a href="https://github.com/fatedier/frp/releases" rel="external nofollow noopener" target="_blank">here</a>, unzip it to <code class="language-plaintext highlighter-rouge">/opt/</code>, and the path should be <code class="language-plaintext highlighter-rouge">/opt/frp_x.xx.x_linux_amd64/</code>. The files of interest are:</p> <ul> <li>frps.ini : server configuration (for host)</li> <li>frps : server executable</li> <li>frpc.ini : client configuration (for containers)</li> <li>frpc : client executable</li> </ul> <h3 id="configuring-frp">Configuring frp</h3> <p>Edit the frps.ini in the host machine</p> <div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[common]</span>
<span class="c"># port of your choice
</span><span class="py">bind_port</span> <span class="p">=</span> <span class="s">7000 </span>
<span class="c"># restric ports for better management
</span><span class="py">allow_ports</span> <span class="p">=</span> <span class="s">2000-3000</span>
</code></pre></div></div> <p>Edit the frpc.ini in the container</p> <div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[common]</span>
<span class="c"># IP of host on lxdbr0
</span><span class="py">server_addr</span> <span class="p">=</span> <span class="s">&lt;host IP of lxdbr0&gt; </span>
<span class="c"># bind_post of host
</span><span class="py">server_port</span> <span class="p">=</span> <span class="s">&lt;bind_port from frps.ini of host&gt;</span>

<span class="c"># an instance, notice the name in [] should be different for different containers
</span><span class="nn">[name]</span>
<span class="py">type</span> <span class="p">=</span> <span class="s">tcp</span>
<span class="py">local_ip</span> <span class="p">=</span> <span class="s">127.0.0.1</span>
<span class="py">local_port</span> <span class="p">=</span> <span class="s">22</span>
<span class="c"># the mapped port on the host
</span><span class="py">remote_port</span> <span class="p">=</span> <span class="s">&lt;port of your choice&gt;</span>
</code></pre></div></div> <h3 id="configuring-frp-to-auto-start-when-booting-the-host-and-launching-the-container">Configuring frp to auto start when booting the host and launching the container</h3> <p>Next we configure frp server (the host) such that it starts automatically.</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">sudo vim /lib/systemd/system/frps.service
</span></code></pre></div></div> <p>Populate it with</p> <pre><code class="language-service">[Unit]
Description=fraps service
After=network.target network-online.target syslog.target
Wants=network.target network-online.target

[Service]
Type=simple

#command to start frp server
ExecStart=/opt/frp_x.xx.x_linux_amd64/frps -c /opt/frp_x.xx.x_linux_amd64/frps.ini

[Install]
WantedBy=multi-user.target
</code></pre> <p>Then run</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">sudo systemctl enable frps
sudo systemctl start frps
</span></code></pre></div></div> <p>It is nearly the same with the frp client (the container).</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>this is <span class="k">in </span>the container
<span class="go">
sudo vim /lib/systemd/system/frpc.service
</span></code></pre></div></div> <p>Populate it with</p> <pre><code class="language-service">[Unit]
Description=fraps service
After=network.target network-online.target syslog.target
Wants=network.target network-online.target

[Service]
Type=simple

#command to start frp client
ExecStart=/opt/frp_x.xx.x_linux_amd64/frpc -c /opt/frp_x.xx.x_linux_amd64/frpc.ini

[Install]
WantedBy=multi-user.target
</code></pre> <p>Then run</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">sudo systemctl enable frpc
sudo systemctl start frpc
</span></code></pre></div></div> <h3 id="connection">Connection</h3> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">ssh -i &lt;private key&gt;</span><span class="w">  </span><span class="nt">-p</span> &lt;remote_port of container frpc&gt; &lt;username, typically ubuntu&gt;@&lt;ip of host&gt;
</code></pre></div></div> <p> </p> <h2 id="creating--deploying-new-containers">Creating &amp; Deploying New Containers</h2> <p>After creating the template, we can now simply copy it and deploy it.</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>create new container
<span class="go">
</span><span class="gp">sudo lxc copy &lt;ContainerTemplateName&gt;</span><span class="w"> </span>&lt;newContainerName&gt;
<span class="gp">sudo lxc start &lt;newContainerName&gt;</span><span class="w">
</span><span class="gp">sudo lxc exec &lt;newContainerName&gt;</span><span class="w"> </span>bash
<span class="go">su ubuntu

</span><span class="gp">#</span><span class="w"> </span>configure new SSH
<span class="go">
cd ~/.ssh/
ssh-keygen -t rsa
</span><span class="gp">cat id_rsa.pub &gt;</span><span class="o">&gt;</span> authorized_keys
<span class="go">sudo service ssh restart
sudo cp id_rsa /your/shared/directory/

</span><span class="gp">#</span><span class="w"> </span>change <span class="nb">hostname</span>
<span class="go">
sudo vim /etc/hostname

</span><span class="gp">#</span><span class="w"> </span>change the old <span class="nb">hostname</span> <span class="o">(</span>the one after 127.0.0.1<span class="o">)</span> to the new one
<span class="go">
sudo vim /etc/hosts

</span><span class="gp">#</span><span class="w"> </span>change the <span class="o">[</span>name] and remote_port of frpc.ini to a different one
<span class="go">sudo vim /opt/frp_x.xx.x_linux_amd64/frpc.ini

</span><span class="gp">#</span><span class="w"> </span>reboot to take effect
<span class="go">
sudo reboot
</span></code></pre></div></div> <p>Take a snapshot,</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">sudo lxc snapshot &lt;ContainerName&gt;</span><span class="w">
</span></code></pre></div></div> <p>To check the name of the snapshot,</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">sudo lxc info &lt;ContainerName&gt;</span><span class="w">
</span></code></pre></div></div> <p>To restore,</p> <div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">sudo lxc restore &lt;ContainerName&gt;</span><span class="w"> </span>&lt;snapshot-name&gt;
</code></pre></div></div> <p> </p> <h2 id="sources">Sources</h2> <p>These sources helped me immensely in this project. Many thanks to these kind people, especially Shen<sup id="a4"><a href="#gpu-server-lab">4</a></sup> and Ewen<sup id="a5"><a href="#frp-service">5</a></sup></p> <p><a name="beaware">1</a> : <a href="https://bayton.org/docs/linux/lxd/lxd-zfs-and-bridged-networking-on-ubuntu-16-04-lts/#be-aware" rel="external nofollow noopener" target="_blank">Be Aware</a> <a href="#a1">↩</a><br> <a name="notty">2</a> : <a href="https://askubuntu.com/questions/1113503/ubuntu-18-04-sudo-no-tty-present-and-no-askpass-program-specified" rel="external nofollow noopener" target="_blank">Sudo Issue</a> <a href="#a2">↩</a><br> <a name="frp">3</a> : <a href="https://github.com/fatedier/frp" rel="external nofollow noopener" target="_blank">frp</a> <a href="#a3">↩</a><br> <a name="gpu-server-lab">4</a> : <a href="https://shenxiaohai.me/2018/12/03/gpu-server-lab/" rel="external nofollow noopener" target="_blank">gpu-server-lab</a> <a href="#a4">↩</a><br> <a name="frp-service">5</a> : <a href="https://blog.csdn.net/sinat_29963957/article/details/83591264" rel="external nofollow noopener" target="_blank">frp-service</a> <a href="#a5">↩</a></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Lizhi Yang. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KZH7TBDT4M"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-KZH7TBDT4M");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>